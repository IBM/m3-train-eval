[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:00,308 >> loading file tokenizer.model from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.model
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:00,309 >> loading file tokenizer.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.json
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:00,309 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:00,309 >> loading file special_tokens_map.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/special_tokens_map.json
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:00,309 >> loading file tokenizer_config.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer_config.json
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:00,309 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:698] 2025-07-08 15:34:00,923 >> loading configuration file config.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json
[INFO|configuration_utils.py:770] 2025-07-08 15:34:00,928 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 32768
}

[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:01,080 >> loading file tokenizer.model from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.model
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:01,080 >> loading file tokenizer.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer.json
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:01,080 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:01,080 >> loading file special_tokens_map.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/special_tokens_map.json
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:01,080 >> loading file tokenizer_config.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/tokenizer_config.json
[INFO|tokenization_utils_base.py:2023] 2025-07-08 15:34:01,080 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:698] 2025-07-08 15:34:01,348 >> loading configuration file config.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/config.json
[INFO|configuration_utils.py:770] 2025-07-08 15:34:01,350 >> Model config MistralConfig {
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": null,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.52.4",
  "use_cache": true,
  "vocab_size": 32768
}

[INFO|modeling_utils.py:1151] 2025-07-08 15:34:02,194 >> loading weights file model.safetensors from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/model.safetensors.index.json
[INFO|modeling_utils.py:2241] 2025-07-08 15:34:02,199 >> Instantiating MistralForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1135] 2025-07-08 15:34:02,203 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:07<00:14,  7.37s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:14<00:07,  7.20s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  6.96s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:21<00:00,  7.04s/it]
[INFO|modeling_utils.py:5131] 2025-07-08 15:34:23,735 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:5139] 2025-07-08 15:34:23,735 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.3.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1090] 2025-07-08 15:34:23,777 >> loading configuration file generation_config.json from cache at /u/aj05/project/hf_cache/hub/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db/generation_config.json
[INFO|configuration_utils.py:1135] 2025-07-08 15:34:23,778 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Environment instance:   0%|          | 0/6 [00:00<?, ?it/s][WARNING|configuration_utils.py:839] 2025-07-08 15:34:35,146 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:34:35,147 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:34:35,150 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:34:42,173 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:34:42,173 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:34:42,173 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:34:51,583 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:34:51,584 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:34:51,584 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:34:58,873 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:34:58,873 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:34:58,873 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Environment instance:  17%|█▋        | 1/6 [00:41<03:25, 41.14s/it][WARNING|configuration_utils.py:839] 2025-07-08 15:35:16,325 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:35:16,325 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:35:16,326 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:35:23,218 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:35:23,218 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:35:23,219 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:35:30,863 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:35:30,864 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:35:30,864 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:35:41,004 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:35:41,004 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:35:41,004 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:37:27,434 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:37:27,434 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:37:27,435 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:37:38,442 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:37:38,442 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:37:38,442 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:39:25,999 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:39:25,999 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:39:26,000 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:39:36,249 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:39:36,249 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:39:36,249 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:39:49,320 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:39:49,320 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:39:49,320 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:41:53,815 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:41:53,816 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:41:53,816 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|logging.py:328] 2025-07-08 15:41:57,328 >> This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (32768). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
Environment instance:  33%|███▎      | 2/6 [07:30<17:12, 258.03s/it][WARNING|configuration_utils.py:839] 2025-07-08 15:42:06,245 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:42:06,245 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:42:06,245 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:42:13,769 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:42:13,769 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:42:13,769 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:42:19,420 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:42:19,420 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:42:19,421 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:42:31,941 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:42:31,941 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:42:31,941 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:42:41,106 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:42:41,106 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:42:41,107 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:42:47,225 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:42:47,225 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:42:47,225 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:42:58,232 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:42:58,233 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:42:58,233 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:43:07,794 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:43:07,794 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:43:07,794 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:43:13,886 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:43:13,886 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:43:13,886 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:43:25,230 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:43:25,231 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:43:25,231 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Environment instance:  50%|█████     | 3/6 [09:00<09:02, 180.87s/it][WARNING|configuration_utils.py:839] 2025-07-08 15:43:35,178 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:43:35,179 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:43:35,179 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:43:41,767 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:43:41,768 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:43:41,768 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:43:49,239 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:43:49,239 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:43:49,240 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:43:56,493 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:43:56,494 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:43:56,494 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Environment instance:  67%|██████▋   | 4/6 [09:34<04:05, 122.92s/it][WARNING|configuration_utils.py:839] 2025-07-08 15:44:09,282 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:44:09,282 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:44:09,282 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:44:14,576 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:44:14,576 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:44:14,576 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:44:20,503 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:44:20,503 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:44:20,504 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
[WARNING|configuration_utils.py:839] 2025-07-08 15:44:26,510 >> The following generation flags are not valid and may be ignored: ['temperature'].
[INFO|configuration_utils.py:840] 2025-07-08 15:44:26,510 >> - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.
[WARNING|utils.py:2150] 2025-07-08 15:44:26,511 >> Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Environment instance:  83%|████████▎ | 5/6 [09:56<01:26, 86.83s/it] Environment instance: 100%|██████████| 6/6 [09:57<00:00, 57.38s/it]Environment instance: 100%|██████████| 6/6 [09:57<00:00, 99.53s/it]
